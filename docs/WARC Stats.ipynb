{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "import os\n",
      "import shutil\n",
      "import math\n",
      "import pprint\n",
      "import datetime\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import cm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compressions = ['gzip', 'bzip2', 'tar.gz', 'tar.bz2']\n",
      "deltas = ['diffe', 'bsdiff', 'vcdiff']\n",
      "delta_strategies = ['first', 'previous', '10']\n",
      "db_root = '/Users/william/Desktop/results'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def churn(query, db='index.db', attach=None):\n",
      "    db_path = os.path.join(db_root, db)\n",
      "    conn = sqlite3.connect(db_path)\n",
      "    cursor = conn.cursor()\n",
      "    if attach is not None:\n",
      "        cursor.execute('ATTACH \"%s\" AS a' % os.path.join(db_root, attach))\n",
      "    cursor.execute(query)\n",
      "    result = cursor.fetchall()\n",
      "    conn.close()\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uri_count = churn('SELECT COUNT(DISTINCT uri) FROM record')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domain_count = churn('SELECT COUNT(DISTINCT job) FROM record')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_response = churn('SELECT COUNT(*) FROM record WHERE record_type = \"response\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request_count = churn('SELECT COUNT(*) FROM record WHERE record_type = \"request\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_date = churn('SELECT MIN(date), round(julianday(MIN(date))) AS INTEGER FROM record WHERE date > \"2008-01-01T00:00:00\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_date = churn('SELECT MAX(date), round(julianday(MAX(date))) AS INTEGER FROM record WHERE date < \"2014-01-01T00:00:00\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_r = churn('SELECT size FROM archivesize WHERE compression_type = \"no_compression\"', db='no_delta.db')\n",
      "raw_warc_sizes = [r[0] for r in _r]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_changes_per_day = churn(\"\"\"\n",
      "    SELECT round(julianday(date)), COUNT(*) \n",
      "    FROM record WHERE record_type=\"response\" \n",
      "    GROUP BY round(julianday(date))\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split = int(churn('SELECT julianday(\"2013-05-05\") - %d' % min_date[0][1])[0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domain_changes_per_day = churn(\"\"\"\n",
      "    SELECT round(julianday(date)), COUNT(DISTINCT job) \n",
      "    FROM record \n",
      "    WHERE record_type=\"response\" \n",
      "    GROUP BY round(julianday(date))\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "change_per_day_by_content_type = churn(\"\"\"\n",
      "    SELECT content_type, AVG(cpd)\n",
      "    FROM\n",
      "        (SELECT content_type, \n",
      "                COUNT(DISTINCT round(julianday(date))) / (round(julianday(MAX(date))) - round(julianday(MIN(date)))) AS cpd\n",
      "        FROM record\n",
      "        WHERE record_type = \"response\"\n",
      "        GROUP BY uri\n",
      "        HAVING round(julianday(MAX(date))) != round(julianday(MIN(date))))\n",
      "    GROUP BY content_type\n",
      "\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-e41fe7615dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         HAVING round(julianday(MAX(date))) != round(julianday(MIN(date))))\n\u001b[1;32m     10\u001b[0m     \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \"\"\")\n\u001b[0m",
        "\u001b[0;32m<ipython-input-3-293c422de981>\u001b[0m in \u001b[0;36mchurn\u001b[0;34m(query, db, attach)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ATTACH \"%s\" AS a'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: An unexpected error occurred while tokenizing input\n",
        "The following traceback may be corrupted or invalid\n",
        "The error message is: ('EOF in multi-line string', (1, 0))\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uri_count_by_domain = churn('SELECT job, COUNT(DISTINCT uri) FROM record GROUP BY job')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_uri_count_by_domain = churn(\"\"\"\n",
      "    SELECT job, AVG(c) \n",
      "    FROM \n",
      "        (SELECT job, COUNT(DISTINCT uri) AS c \n",
      "        FROM record \n",
      "        GROUP BY job, round(julianday(date))) \n",
      "    GROUP BY job\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content_type_counts = churn('SELECT content_type, COUNT(DISTINCT uri) FROM record GROUP BY content_type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'SELECT compression_type, SUM(size) FROM archivesize GROUP BY compression_type'\n",
      "archive_sizes = {}\n",
      "for db in os.walk(db_root).next()[2]:\n",
      "    if db.endswith('.db') and db != 'index.db':\n",
      "        name = db[:-3]\n",
      "        archive_sizes[name] = {}\n",
      "        for ct, s in churn(sql, db=db):\n",
      "            archive_sizes[name][ct] = s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nc = map(lambda t: (t[0], t[1]['no_compression']), archive_sizes.iteritems())\n",
      "at10 = sum(map(lambda t: t[1], filter(lambda t: '@10' in t[0], nc)))\n",
      "previous = sum(map(lambda t: t[1], filter(lambda t: '@previous' in t[0], nc)))\n",
      "at10_overheads_previous = float(at10) / previous - 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first = archive_sizes['bsdiff@first']['no_compression']\n",
      "previous = archive_sizes['bsdiff@previous']['no_compression']\n",
      "bsdiff_first_overheads_previous = float(first) / previous - 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_record_sizes = {}\n",
      "for r in churn('SELECT record_type, AVG(content_length) FROM record GROUP BY record_type'):\n",
      "    mean_record_sizes[r[0]] = r[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'SELECT AVG(content_length) FROM recordsize'\n",
      "mean_record_sizes['delta'] = {}\n",
      "for db in os.walk(db_root).next()[2]:\n",
      "    if db.endswith('.db') and db not in ['index.db', 'no_delta.db']:\n",
      "        name = db[:-3]\n",
      "        s, c = churn('SELECT SUM(content_length), COUNT(*) FROM recordsize', db=db)[0]\n",
      "        ms, mc = churn('SELECT SUM(content_length), COUNT(*) FROM record WHERE record.record_id NOT IN (SELECT record_id FROM a.recordsize) AND record_type=\"response\"', attach=db)[0]\n",
      "        mean_record_sizes['delta'][name] = float(s + ms) / (c + mc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "labels, heights = zip(*sorted([(k, v) for k, v in mean_record_sizes.iteritems() if k != 'delta'], key=lambda x: x[1]))\n",
      "dlabels, dheights = zip(*sorted([(k, v) for k, v in mean_record_sizes['delta'].iteritems()], key=lambda x: x[1], reverse=True))\n",
      "labels += dlabels\n",
      "heights += dheights\n",
      "plt.bar(xrange(len(heights)), heights)\n",
      "plt.axis([0, len(heights), 0, max(heights)])\n",
      "plt.xlabel('Record Type')\n",
      "plt.ylabel('Mean Record Size (bytes)')\n",
      "plt.xticks([i + 0.5 for i in xrange(len(heights))], labels, rotation='vertical')\n",
      "plt.gcf().set_size_inches(6,5)\n",
      "plt.subplots_adjust(bottom=0.35, left=0.2)\n",
      "plt.savefig('images/record_type_mean_size.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "x1 = [r[0] - min_date[0][1] for r in file_changes_per_day if r[0] - min_date[0][1] < split]\n",
      "y1 = [r[1] for r in file_changes_per_day if r[0] - min_date[0][1] < split]\n",
      "plt.plot(x1, y1, 'r.')\n",
      "plt.axis([0, split, 0, max(y1)+10])\n",
      "plt.xlabel('Day')\n",
      "plt.ylabel('Number of changed files')\n",
      "plt.savefig('images/file_changes_per_day_first.png')\n",
      "\n",
      "plt.clf()\n",
      "x2 = [r[0] - min_date[0][1] for r in file_changes_per_day if r[0] - min_date[0][1] >= split]\n",
      "y2 = [r[1] for r in file_changes_per_day if r[0] - min_date[0][1] >= split]\n",
      "plt.plot(x2, y2, 'r.')\n",
      "plt.axis([split, max_date[0][1] - min_date[0][1], 0, max(y2)+50])\n",
      "plt.xlabel('Day')\n",
      "plt.ylabel('Number of changed files')\n",
      "plt.savefig('images/file_changes_per_day_last.png')\n",
      "\n",
      "mean_file_changes_first = float(sum(y1)) / split\n",
      "mean_file_changes_last = float(sum(y2)) / (max_date[0][1] - min_date[0][1] - split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "x1 = [r[0] - min_date[0][1] for r in domain_changes_per_day if r[0] - min_date[0][1] < split]\n",
      "y1 = [r[1] for r in domain_changes_per_day if r[0] - min_date[0][1] < split]\n",
      "plt.plot(x1, y1, 'r.')\n",
      "plt.axis([0, split, 0, max(y1)])\n",
      "plt.xlabel('Day')\n",
      "plt.ylabel('Number of domain changes')\n",
      "plt.savefig('images/domain_changes_per_day_first.png')\n",
      "\n",
      "plt.clf()\n",
      "x2 = [r[0] - min_date[0][1] for r in domain_changes_per_day if r[0] - min_date[0][1] >= split]\n",
      "y2 = [r[1] for r in domain_changes_per_day if r[0] - min_date[0][1] >= split]\n",
      "plt.plot(x2, y2, 'r.')\n",
      "plt.axis([split, max_date[0][1] - min_date[0][1], 0, max(y2)])\n",
      "plt.xlabel('Day')\n",
      "plt.ylabel('Number of domain changes')\n",
      "plt.savefig('images/domain_changes_per_day_last.png')\n",
      "\n",
      "mean_domain_changes_first = float(sum(y1)) / split\n",
      "mean_domain_changes_last = float(sum(y2)) / (max_date[0][1] - min_date[0][1] - split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "_s = sorted(uri_count_by_domain, reverse=True, key=lambda x: x[1])\n",
      "heights = [math.log(r[1]) for r in _s]\n",
      "plt.plot(list(xrange(len(heights))), heights, 'r.')\n",
      "plt.axis([0, len(heights), 0, max(heights)])\n",
      "plt.xlabel('Domain')\n",
      "plt.ylabel('Number of URIs (log)')\n",
      "plt.suptitle('Number of URIs per domain')\n",
      "plt.savefig('images/uris_per_domain.png')\n",
      "\n",
      "largest_domain = [_s[0][0], _s[0][1], None]\n",
      "smallest_domain = [_s[-1][0], _s[-1][1], None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "_s = sorted(avg_uri_count_by_domain, reverse=True, key=lambda x: x[1])\n",
      "heights = [math.log(r[1]) for r in _s]\n",
      "plt.plot(list(xrange(len(heights))), heights, 'r.')\n",
      "plt.axis([0, len(heights), 0, max(heights)])\n",
      "plt.xlabel('Domain')\n",
      "plt.ylabel('Average number of URIs (log)')\n",
      "plt.suptitle('Average number of URIs per domain')\n",
      "plt.savefig('images/uris_per_domain_avg.png')\n",
      "\n",
      "largest_domain[2] = _s[0][1]\n",
      "smallest_domain[2] = _s[-1][1]\n",
      "\n",
      "if largest_domain[0] != _s[0][0]:\n",
      "    print 'Largest domains not the same'\n",
      "if smallest_domain[0] != _s[-1][0]:\n",
      "    print 'Smallest domains not the same'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "counts = sorted(filter(lambda x: x[0] is not None, content_type_counts), reverse=True, key=lambda x: x[1])\n",
      "total = sum(map(lambda x: x[1], counts))\n",
      "bars = [float(r[1]) / total for r in counts]\n",
      "labels = [r[0].strip() for r in counts]\n",
      "plt.bar(xrange(len(bars)), bars)\n",
      "plt.axis([0, len(bars), 0, 1])\n",
      "plt.xlabel('Content Type')\n",
      "plt.ylabel('Number of Distinct URIs')\n",
      "plt.suptitle('The number of distinct URIs of each content type')\n",
      "plt.xticks([i + 0.5 for i in xrange(len(bars))], labels, rotation='vertical')\n",
      "plt.gcf().set_size_inches(5,5)\n",
      "plt.subplots_adjust(bottom=0.6)\n",
      "plt.savefig('images/uris_by_content_type.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "bars = sorted([(d, s['no_compression']) for d, s in archive_sizes.iteritems()], reverse=True, key=lambda x: x[1])\n",
      "plt.bar(xrange(len(bars)), [x[1] for x in bars])\n",
      "plt.axis([0, len(bars), 0, max([x[1] for x in bars])])\n",
      "plt.ylabel('Total Archive Size (bytes)')\n",
      "plt.xticks([i + 0.5 for i in xrange(len(bars))], [x[0] for x in bars], rotation='vertical')\n",
      "plt.gcf().set_size_inches(5,5)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig('images/tas_delta.png')\n",
      "\n",
      "delta_max_size = max(bars, key=lambda x: x[1])\n",
      "delta_min_size = min(bars, key=lambda x: x[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "bars = sorted([(c, s) for c, s in archive_sizes['no_delta'].iteritems()], reverse=True, key=lambda x: x[1])\n",
      "plt.bar(xrange(len(bars)), [x[1] for x in bars])\n",
      "plt.axis([0, len(bars), 0, max([x[1] for x in bars])])\n",
      "plt.ylabel('Total Archive Size (bytes)')\n",
      "plt.xticks([i + 0.5 for i in xrange(len(bars))], [x[0] for x in bars], rotation='vertical')\n",
      "plt.gcf().set_size_inches(5,5)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig('images/tas_compression.png')\n",
      "\n",
      "comp_max_size = max(bars, key=lambda x: x[1])\n",
      "comp_min_size = min(bars, key=lambda x: x[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.clf()\n",
      "nothing = archive_sizes['no_delta']['no_compression']\n",
      "recommended = archive_sizes['no_delta']['gzip']\n",
      "best = ('Error', float('inf'))\n",
      "for d in archive_sizes:\n",
      "    for c in archive_sizes[d]:\n",
      "        if archive_sizes[d][c] < best[1]:\n",
      "            best = ('%s:%s' % (d, c), archive_sizes[d][c])\n",
      "plt.bar([0, 1, 2, 3, 4], [nothing, delta_min_size[1], recommended, comp_min_size[1], best[1]])\n",
      "plt.axis([0, 5, 0, nothing])\n",
      "plt.ylabel('Total Archive Size (bytes)')\n",
      "plt.xticks([0.5, 1.5, 2.5, 3.5, 4.5], ['original', delta_min_size[0], 'GZip only', comp_min_size[0], best[0]], rotation='vertical')\n",
      "plt.gcf().set_size_inches(5,5)\n",
      "plt.subplots_adjust(bottom=0.35)\n",
      "plt.savefig('images/tas_best.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def commaify(x):\n",
      "    if type(x) not in [type(0), type(0L)]:\n",
      "        raise TypeError(\"Parameter must be an integer.\")\n",
      "    if x < 0:\n",
      "        return '-' + commaify(-x)\n",
      "    result = ''\n",
      "    while x >= 1000:\n",
      "        x, r = divmod(x, 1000)\n",
      "        result = \",%03d%s\" % (r, result)\n",
      "    return \"%d%s\" % (x, result)\n",
      "\n",
      "def nice_date(d):\n",
      "    d = d.rstrip('Z')\n",
      "    return datetime.datetime.strptime(d,'%Y-%m-%dT%H:%M:%S').strftime('%d %B %Y')\n",
      "\n",
      "def strat_name(s):\n",
      "    d, l = s.split('@', 1)\n",
      "    try:\n",
      "        return '%s with a reference record every %d records' % (d, int(l))\n",
      "    except:\n",
      "        return '%s comparing against the %s record' % (d, l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\"\"\n",
      "\\def \\gzipcompressionpct {{{gzip:.5f}}}\n",
      "\\def \\\\bzipcompressionpct {{{bzip2:.5f}}}\n",
      "\\def \\\\beststrategypct {{{best_strat:.5f}}}\n",
      "\\def \\\\bestdeltapct {{{best_delta_pct:.5f}}}\n",
      "\\def \\\\bestdeltaname {{{best_delta_name}}}\n",
      "\\def \\\\bestvsgzippct {{{best_vs_gzip:.5f}}}\n",
      "\\def \\\\bzipvsgzippct {{{bzip_vs_gzip:.5f}}}\n",
      "\\def \\domainscrawled {{{domainscrawled}}}\n",
      "\\def \\uniqueuris {{{uniqueuris}}}\n",
      "\\def \\\\requestssent {{{requestssent}}}\n",
      "\\def \\uniqueresponses {{{uniqueresponses}}}\n",
      "\\def \\oldestpage {{{oldestpage}}}\n",
      "\\def \\youngestpage {{{youngestpage}}}\n",
      "\\def \\\\totaldays {{{totaldays}}}\n",
      "\\def \\iostarted {{{io_started}}}\n",
      "\\def \\\\afterio {{{after_io}}}\n",
      "\\def \\largewarccount {{{large_warc_count}}}\n",
      "\\def \\meanwarcsize {{{mean_warc_size}}}\n",
      "\\def \\maxwarcsize {{{max_warc_size}}}\n",
      "\\def \\minwarcsize {{{min_warc_size}}}\n",
      "\\def \\\\newiasize {{{new_ia_size:.5f}}}\n",
      "\\def \\\\newglaciercost {{{new_glacier_cost}}}\n",
      "\\def \\\\newglaciersavings {{{glacier_savings}}}\n",
      "\\def \\largestdomainname {{{largest_domain_name}}}\n",
      "\\def \\largestdomaintotal {{{largest_domain_total}}}\n",
      "\\def \\largestdomainmean {{{largest_domain_mean}}}\n",
      "\\def \\smallestdomainname {{{smallest_domain_name}}}\n",
      "\\def \\smallestdomaintotal {{{smallest_domain_total}}}\n",
      "\\def \\smallestdomainmean {{{smallest_domain_mean}}}\n",
      "\\def \\meanfilechangesfirst {{{mean_file_changes_first:.5f}}}\n",
      "\\def \\meanfilechangeslast {{{mean_file_changes_last:.5f}}}\n",
      "\\def \\meandomainchangesfirst {{{mean_domain_changes_first:.5f}}}\n",
      "\\def \\meandomainchangeslast {{{mean_domain_changes_last:.5f}}}\n",
      "\\def \\\\referenceframeoverhead {{{at10_overheads_previous:.5f}}}\n",
      "\\def \\\\bsdifffirstoverhead {{{bsdiff_first_overheads_previous:.5f}}}\n",
      "\"\"\".format(\n",
      "    gzip=(100.0 * archive_sizes['no_delta']['gzip'] / comp_max_size[1]),\n",
      "    bzip2=(100.0 * archive_sizes['no_delta']['bz2'] / comp_max_size[1]),\n",
      "    bzip_vs_gzip=(100.0 * archive_sizes['no_delta']['bz2'] / recommended),\n",
      "    best_delta_pct=(100.0 * delta_min_size[1] / delta_max_size[1]),\n",
      "    best_delta_name=strat_name(delta_min_size[0]),\n",
      "    best_strat=(100.0 * best[1] / nothing),\n",
      "    best_vs_gzip=(100.0 * best[1] / recommended),\n",
      "    domainscrawled=commaify(domain_count[0][0]),\n",
      "    uniqueuris=commaify(uri_count[0][0]),\n",
      "    requestssent=commaify(request_count[0][0]),\n",
      "    uniqueresponses=commaify(unique_response[0][0]),\n",
      "    oldestpage=nice_date(min_date[0][0]),\n",
      "    youngestpage=nice_date(max_date[0][0]),\n",
      "    totaldays=commaify(int(max_date[0][1] - min_date[0][1])),\n",
      "    large_warc_count=commaify(len(filter(lambda s: s > 1024*1024*1024, raw_warc_sizes))),\n",
      "    mean_warc_size=commaify(sum(raw_warc_sizes) / len(raw_warc_sizes)),\n",
      "    min_warc_size=commaify(min(raw_warc_sizes)),\n",
      "    max_warc_size=commaify(max(raw_warc_sizes)),\n",
      "    new_ia_size=(2.0 * best[1] / recommended),\n",
      "    new_glacier_cost=commaify(int(20972.0 * best[1] / recommended)),\n",
      "    glacier_savings=commaify(20972 - int(20972.0 * best[1] / recommended)),\n",
      "    io_started=commaify(split),\n",
      "    after_io=commaify(int(max_date[0][1] - min_date[0][1]) - split),\n",
      "    mean_file_changes_first=mean_file_changes_first,\n",
      "    mean_file_changes_last=mean_file_changes_last,\n",
      "    mean_domain_changes_first=mean_domain_changes_first,\n",
      "    mean_domain_changes_last=mean_domain_changes_last,\n",
      "    largest_domain_name=largest_domain[0],\n",
      "    largest_domain_total=commaify(largest_domain[1]),\n",
      "    largest_domain_mean=commaify(int(largest_domain[2] + 0.5)),\n",
      "    smallest_domain_name=smallest_domain[0],\n",
      "    smallest_domain_total=commaify(smallest_domain[1]),\n",
      "    smallest_domain_mean=commaify(int(smallest_domain[2] + 0.5)),\n",
      "    at10_overheads_previous=100 * at10_overheads_previous,\n",
      "    bsdiff_first_overheads_previous=100 * bsdiff_first_overheads_previous)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}